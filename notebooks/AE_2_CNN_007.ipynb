{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.utils import plot_model, to_categorical\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D, UpSampling2D, BatchNormalization, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_randvalue(value):\n",
    "    # Set a seed value\n",
    "    seed_value= value \n",
    "    # 1. Set `PYTHONHASHSEED` environment variable at a fixed value\n",
    "    os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "    # 2. Set `python` built-in pseudo-random generator at a fixed value\n",
    "    random.seed(seed_value)\n",
    "    # 3. Set `numpy` pseudo-random generator at a fixed value\n",
    "    np.random.seed(seed_value)\n",
    "    # 4. Set `tensorflow` pseudo-random generator at a fixed value\n",
    "    tf.random.set_seed(seed_value)\n",
    "\n",
    "set_randvalue(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preprocessing and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data() # load data\n",
    "x_train,x_test = x_train.astype('float32')/255.0,x_test.astype('float32')/255.0 # normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [9],\n",
       "       [9],\n",
       "       ...,\n",
       "       [9],\n",
       "       [1],\n",
       "       [1]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit three class preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bird label num is 2500\n",
      "Deer label num is 2500\n",
      "Truck label num is 2500\n",
      "Other label num is 35000\n",
      "Train label num is 42500\n",
      "(42500, 32, 32, 3)\n",
      "(42500,)\n"
     ]
    }
   ],
   "source": [
    "# No method on keras to get cifar10 category label name by categoly label?\n",
    "cifar10_labels = np.array([\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'])\n",
    "\n",
    "bird_num = np.where(cifar10_labels=='bird')\n",
    "deer_num = np.where(cifar10_labels=='deer')\n",
    "truck_num = np.where(cifar10_labels=='truck')\n",
    "\n",
    "limit_num = 2500\n",
    "\n",
    "# get limit label indexes\n",
    "bird_indexes = [i for i, label in enumerate(y_train) if label == bird_num]\n",
    "deer_indexes = [i for i, label in enumerate(y_train) if label == deer_num] \n",
    "truck_indexes = [i for i, label in enumerate(y_train) if label == truck_num] \n",
    "other_indexes = [i for i, label in enumerate(y_train) if label not in [bird_num, deer_num, truck_num]]\n",
    "\n",
    "# limit\n",
    "bird_indexes = bird_indexes[:limit_num]\n",
    "deer_indexes = deer_indexes[:limit_num]\n",
    "truck_indexes = truck_indexes[:limit_num]\n",
    "print(f'Bird label num is {len(bird_indexes)}') # 2500\n",
    "print(f'Deer label num is {len(deer_indexes)}') # 2500\n",
    "print(f'Truck label num is {len(truck_indexes)}') # 2500\n",
    "print(f'Other label num is {len(other_indexes)}') # 35000; 5000*7\n",
    "\n",
    "# merge and sort\n",
    "merge_indexes = np.concatenate([other_indexes, bird_indexes, deer_indexes, truck_indexes], 0)\n",
    "merge_indexes.sort()\n",
    "print(f'Train label num is {len(merge_indexes)}') # 42500\n",
    "\n",
    "# create three labels removed train data\n",
    "x_train_removed =  np.zeros((len(merge_indexes), 32, 32, 3))\n",
    "y_train_removed =  np.zeros(len(merge_indexes))\n",
    "\n",
    "for i, train_index in enumerate(merge_indexes):\n",
    "    x_train_removed[i] = x_train[train_index]\n",
    "    y_train_removed[i] = y_train[train_index]\n",
    "    \n",
    "print(x_train_removed.shape)\n",
    "print(y_train_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train.shape\n",
    "# df_train = pd.DataFrame(y_train, columns=['label'])\n",
    "# index = df_train[df_train['label'] == 2].index\n",
    "# len(index)\n",
    "bird_num[0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label num is 7500\n",
      "Other label num is 35000\n"
     ]
    }
   ],
   "source": [
    "# merge and sort\n",
    "target_indexes = np.concatenate([bird_indexes, deer_indexes, truck_indexes], 0)\n",
    "target_indexes.sort()\n",
    "print(f'Target label num is {len(target_indexes)}') # 7500\n",
    "print(f'Other label num is {len(other_indexes)}') # 35000\n",
    "\n",
    "# create three labels removed train data\n",
    "x_train_target =  np.zeros((len(target_indexes), 32, 32, 3))\n",
    "y_train_target =  np.zeros(len(target_indexes))\n",
    "\n",
    "for i, train_index in enumerate(target_indexes):\n",
    "    x_train_target[i] = x_train[train_index]\n",
    "    y_train_target[i] = y_train[train_index]\n",
    "\n",
    "x_train_other =  np.zeros((len(other_indexes), 32, 32, 3))\n",
    "y_train_other =  np.zeros(len(other_indexes))\n",
    "\n",
    "for i, train_index in enumerate(other_indexes):\n",
    "    x_train_other[i] = x_train[train_index]\n",
    "    y_train_other[i] = y_train[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0    5000\n",
      "7.0    5000\n",
      "6.0    5000\n",
      "5.0    5000\n",
      "3.0    5000\n",
      "1.0    5000\n",
      "0.0    5000\n",
      "9.0    2500\n",
      "4.0    2500\n",
      "2.0    2500\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(y_train_removed.flatten())\n",
    "df = pd.DataFrame(y_train_removed.flatten())\n",
    "print(df.value_counts())\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5000., 5000., 2500., 5000., 2500., 5000., 5000., 5000., 5000.,\n",
       "        2500.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot data labels\n",
    "plt.hist(y_train_removed.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "# stratify y label\n",
    "x_train_target_, x_valid_target_, y_train_target_, y_valid_target_ = train_test_split(x_train_target, y_train_target, \n",
    "                                                                      test_size=0.3, random_state=42, stratify=y_train_target)\n",
    "x_train_other_, x_valid_other_, y_train_other_, y_valid_other_ = train_test_split(x_train_other, y_train_other, \n",
    "                                                                      test_size=0.3, random_state=42, stratify=y_train_other)\n",
    "x_train_other__, x_valid_other__, y_train_other__, y_valid_other__ = train_test_split(x_train_other_, y_train_other_, \n",
    "                                                                     test_size=0.5, random_state=42, stratify=y_train_other_)\n",
    "\n",
    "# Train set 1\n",
    "x_train_removed1 = np.concatenate([x_train_target_, x_train_other__],0)\n",
    "x_valid_removed1 = np.concatenate([x_valid_target_, x_valid_other_],0)\n",
    "y_train_removed1 = np.concatenate([y_train_target_, y_train_other__],0)\n",
    "y_valid_removed1 = np.concatenate([y_valid_target_, y_valid_other_],0)\n",
    "\n",
    "# Train set 2\n",
    "x_train_removed2 = np.concatenate([x_train_target_, x_valid_other__],0)\n",
    "x_valid_removed2 = np.concatenate([x_valid_target_, x_train_other_],0)\n",
    "y_train_removed2 = np.concatenate([y_train_target_, y_valid_other__],0)\n",
    "y_valid_removed2 = np.concatenate([y_valid_target_, y_train_other_],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17500, 32, 32, 3)\n",
      "(17500,)\n",
      "(12750, 32, 32, 3)\n",
      "(12750,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_removed1.shape)\n",
    "print(y_train_removed1.shape)\n",
    "print(x_valid_removed1.shape)\n",
    "print(y_valid_removed1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0    1750\n",
      "8.0    1750\n",
      "7.0    1750\n",
      "6.0    1750\n",
      "5.0    1750\n",
      "4.0    1750\n",
      "3.0    1750\n",
      "2.0    1750\n",
      "1.0    1750\n",
      "0.0    1750\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(y_train_removed1.flatten())\n",
    "print(df.value_counts())\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0    1500\n",
      "7.0    1500\n",
      "6.0    1500\n",
      "5.0    1500\n",
      "3.0    1500\n",
      "1.0    1500\n",
      "0.0    1500\n",
      "9.0     750\n",
      "4.0     750\n",
      "2.0     750\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(y_valid_removed1.flatten())\n",
    "print(df.value_counts())\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load AE models weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Norm Model\n",
    "def create_AE01_model(k_size):\n",
    "    input_img = Input(shape=(32, 32, 3))  # 0\n",
    "    conv1 = Conv2D(64, (k_size, k_size), padding='same', name=\"Dense_AE01_1\")(input_img) # 1\n",
    "    conv1 = BatchNormalization(name=\"BN_AE01_1\")(conv1) # 2\n",
    "    conv1 = Activation('relu', name=\"Relu_AE01_1\")(conv1) # 3\n",
    "        \n",
    "    decoded = Conv2D(3, (k_size, k_size), padding='same', name=\"Dense_AE01_2\")(conv1) # 4\n",
    "    decoded = BatchNormalization(name=\"BN_AE01_2\")(decoded) # 5\n",
    "    decoded = Activation('relu', name=\"Relu_AE01_2\")(decoded) # 6\n",
    "    return Model(input_img, decoded)\n",
    "\n",
    "class AE01():\n",
    "    def __init__(self, ksize, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        \n",
    "        self.autoencoder = create_AE01_model(ksize)\n",
    "        self.encoder = None\n",
    "\n",
    "    def compile(self, optimizer='adam', loss='binary_crossentropy'):\n",
    "        self.autoencoder.compile(optimizer=self.optimizer, loss=loss)\n",
    "\n",
    "    def train(self, x_train=None, x_test=None, epochs=1, batch_size=32, shuffle=True):\n",
    "        es_cb = EarlyStopping(monitor='val_loss', patience=2, verbose=1, mode='auto')\n",
    "        ae_model_path = '../models/AE/AE01_AE_Best.hdf5'\n",
    "        cp_cb = ModelCheckpoint(filepath = ae_model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "       \n",
    "        history = self.autoencoder.fit(x_train, x_train,\n",
    "                             epochs=epochs,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=shuffle,\n",
    "                             callbacks=[es_cb, cp_cb],\n",
    "                             validation_data=(x_test, x_test))\n",
    "        \n",
    "        self.autoencoder.load_weights(ae_model_path)\n",
    "        \n",
    "        self.encoder = Model(self.autoencoder.input, self.autoencoder.get_layer('Relu_AE01_1').output)\n",
    "        encode_model_path = '../models/AE/AE01_Encoder_Best.hdf5'\n",
    "        self.encoder.save(encode_model_path)\n",
    "        return history\n",
    "    \n",
    "    def load_weights(self, ae_model_path, encode_model_path):\n",
    "        self.autoencoder.load_weights(ae_model_path)\n",
    "        self.encoder = Model(self.autoencoder.input, self.autoencoder.get_layer('Relu_AE01_1').output)\n",
    "        self.encoder.load_weights(encode_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "Dense_AE01_1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "BN_AE01_1 (BatchNormalizatio (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "Relu_AE01_1 (Activation)     (None, 32, 32, 64)        0         \n",
      "=================================================================\n",
      "Total params: 2,048\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "ae_ksize = 3\n",
    "ae_optimizer = 'rmsprop'\n",
    "stack01 = AE01(ae_ksize, ae_optimizer)\n",
    "stack01.load_weights('../models/AE/AE01_AE_Best.hdf5', '../models/AE/AE01_Encoder_Best.hdf5')\n",
    "stack01.encoder.trainable = False\n",
    "stack01.encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Model AE to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_StackedAE01_CNN01_model(encoder):\n",
    "    input_img = encoder.input\n",
    "    output = encoder.layers[-1].output # 32,32,64\n",
    "    x = Conv2D(64,(3,3),padding = \"same\",activation= \"relu\")(output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) # 16,16,64\n",
    "    \n",
    "    x = Conv2D(128,(3,3),padding = \"same\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Conv2D(128,(3,3),padding = \"same\",activation= \"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x) # 8,8,128\n",
    "    \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation(\"relu\")(x)\n",
    "    y = Dense(10,activation = \"softmax\")(x)\n",
    "\n",
    "    return Model(input_img, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifar10_labels = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck']\n",
    "\n",
    "cifar10_labels.index('airplane')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under sampling without data augumentation & Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label num is 1666, and total label num is 16661\n",
      "Epoch 1/400\n",
      "521/521 [==============================] - ETA: 0s - loss: 1.5675 - accuracy: 0.4286\n",
      "Epoch 00001: val_loss improved from inf to 1.72293, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 4s 7ms/step - loss: 1.5675 - accuracy: 0.4286 - val_loss: 1.7229 - val_accuracy: 0.4216\n",
      "Epoch 2/400\n",
      "512/521 [============================>.] - ETA: 0s - loss: 1.2121 - accuracy: 0.5614\n",
      "Epoch 00002: val_loss improved from 1.72293 to 1.46938, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 1.2111 - accuracy: 0.5613 - val_loss: 1.4694 - val_accuracy: 0.4888\n",
      "Epoch 3/400\n",
      "510/521 [============================>.] - ETA: 0s - loss: 1.0514 - accuracy: 0.6223\n",
      "Epoch 00003: val_loss improved from 1.46938 to 0.99888, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 1.0517 - accuracy: 0.6231 - val_loss: 0.9989 - val_accuracy: 0.6469\n",
      "Epoch 4/400\n",
      "512/521 [============================>.] - ETA: 0s - loss: 0.9053 - accuracy: 0.6782\n",
      "Epoch 00004: val_loss improved from 0.99888 to 0.91121, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.9032 - accuracy: 0.6789 - val_loss: 0.9112 - val_accuracy: 0.6773\n",
      "Epoch 5/400\n",
      "510/521 [============================>.] - ETA: 0s - loss: 0.7790 - accuracy: 0.7219\n",
      "Epoch 00005: val_loss did not improve from 0.91121\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.7809 - accuracy: 0.7215 - val_loss: 1.1437 - val_accuracy: 0.6173\n",
      "Epoch 6/400\n",
      "516/521 [============================>.] - ETA: 0s - loss: 0.6743 - accuracy: 0.7628\n",
      "Epoch 00006: val_loss did not improve from 0.91121\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.6752 - accuracy: 0.7628 - val_loss: 0.9414 - val_accuracy: 0.6884\n",
      "Epoch 7/400\n",
      "515/521 [============================>.] - ETA: 0s - loss: 0.5850 - accuracy: 0.7917\n",
      "Epoch 00007: val_loss improved from 0.91121 to 0.90227, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 4s 7ms/step - loss: 0.5860 - accuracy: 0.7915 - val_loss: 0.9023 - val_accuracy: 0.6948\n",
      "Epoch 8/400\n",
      "510/521 [============================>.] - ETA: 0s - loss: 0.4868 - accuracy: 0.8284\n",
      "Epoch 00008: val_loss improved from 0.90227 to 0.84452, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.4863 - accuracy: 0.8288 - val_loss: 0.8445 - val_accuracy: 0.7314\n",
      "Epoch 9/400\n",
      "510/521 [============================>.] - ETA: 0s - loss: 0.4139 - accuracy: 0.8555\n",
      "Epoch 00009: val_loss improved from 0.84452 to 0.82735, saving model to ../models/CNN\\Model_015_1_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.4148 - accuracy: 0.8548 - val_loss: 0.8273 - val_accuracy: 0.7435\n",
      "Epoch 10/400\n",
      "514/521 [============================>.] - ETA: 0s - loss: 0.3358 - accuracy: 0.8794\n",
      "Epoch 00010: val_loss did not improve from 0.82735\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.3379 - accuracy: 0.8788 - val_loss: 0.9448 - val_accuracy: 0.7367\n",
      "Epoch 11/400\n",
      "514/521 [============================>.] - ETA: 0s - loss: 0.2865 - accuracy: 0.8990\n",
      "Epoch 00011: val_loss did not improve from 0.82735\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.2874 - accuracy: 0.8986 - val_loss: 0.9172 - val_accuracy: 0.7555\n",
      "Epoch 12/400\n",
      "516/521 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9187\n",
      "Epoch 00012: val_loss did not improve from 0.82735\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.2285 - accuracy: 0.9185 - val_loss: 1.0651 - val_accuracy: 0.7313\n",
      "Epoch 00012: early stopping\n",
      "443/443 [==============================] - 1s 3ms/step - loss: 0.8273 - accuracy: 0.7435\n",
      "Score for fold 1: loss of 0.8273465633392334; accuracy of 74.34883713722229%\n",
      "Target label num is 1667, and total label num is 16670\n",
      "Epoch 1/400\n",
      "521/521 [==============================] - ETA: 0s - loss: 1.5662 - accuracy: 0.4329\n",
      "Epoch 00001: val_loss improved from inf to 1.50071, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 4s 8ms/step - loss: 1.5662 - accuracy: 0.4329 - val_loss: 1.5007 - val_accuracy: 0.4925\n",
      "Epoch 2/400\n",
      "512/521 [============================>.] - ETA: 0s - loss: 1.2206 - accuracy: 0.5626\n",
      "Epoch 00002: val_loss improved from 1.50071 to 1.28913, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 1.2204 - accuracy: 0.5630 - val_loss: 1.2891 - val_accuracy: 0.5366\n",
      "Epoch 3/400\n",
      "512/521 [============================>.] - ETA: 0s - loss: 1.0283 - accuracy: 0.6348\n",
      "Epoch 00003: val_loss improved from 1.28913 to 0.99728, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 1.0279 - accuracy: 0.6347 - val_loss: 0.9973 - val_accuracy: 0.6526\n",
      "Epoch 4/400\n",
      "514/521 [============================>.] - ETA: 0s - loss: 0.8883 - accuracy: 0.6842\n",
      "Epoch 00004: val_loss improved from 0.99728 to 0.98134, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 4s 7ms/step - loss: 0.8877 - accuracy: 0.6847 - val_loss: 0.9813 - val_accuracy: 0.6583\n",
      "Epoch 5/400\n",
      "521/521 [==============================] - ETA: 0s - loss: 0.7571 - accuracy: 0.7333\n",
      "Epoch 00005: val_loss improved from 0.98134 to 0.89735, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.7571 - accuracy: 0.7333 - val_loss: 0.8974 - val_accuracy: 0.6897\n",
      "Epoch 6/400\n",
      "515/521 [============================>.] - ETA: 0s - loss: 0.6547 - accuracy: 0.7704\n",
      "Epoch 00006: val_loss did not improve from 0.89735\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.6545 - accuracy: 0.7700 - val_loss: 0.9322 - val_accuracy: 0.6922\n",
      "Epoch 7/400\n",
      "519/521 [============================>.] - ETA: 0s - loss: 0.5557 - accuracy: 0.8062\n",
      "Epoch 00007: val_loss improved from 0.89735 to 0.89237, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.5551 - accuracy: 0.8063 - val_loss: 0.8924 - val_accuracy: 0.7122\n",
      "Epoch 8/400\n",
      "514/521 [============================>.] - ETA: 0s - loss: 0.4779 - accuracy: 0.8303\n",
      "Epoch 00008: val_loss improved from 0.89237 to 0.72066, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.4785 - accuracy: 0.8301 - val_loss: 0.7207 - val_accuracy: 0.7683\n",
      "Epoch 9/400\n",
      "520/521 [============================>.] - ETA: 0s - loss: 0.3857 - accuracy: 0.8651\n",
      "Epoch 00009: val_loss improved from 0.72066 to 0.70534, saving model to ../models/CNN\\Model_015_2_Best.hdf5\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.3856 - accuracy: 0.8651 - val_loss: 0.7053 - val_accuracy: 0.7782\n",
      "Epoch 10/400\n",
      "520/521 [============================>.] - ETA: 0s - loss: 0.3133 - accuracy: 0.8901\n",
      "Epoch 00010: val_loss did not improve from 0.70534\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.3139 - accuracy: 0.8899 - val_loss: 1.0381 - val_accuracy: 0.7215\n",
      "Epoch 11/400\n",
      "515/521 [============================>.] - ETA: 0s - loss: 0.2639 - accuracy: 0.9071\n",
      "Epoch 00011: val_loss did not improve from 0.70534\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.2636 - accuracy: 0.9071 - val_loss: 0.9992 - val_accuracy: 0.7430\n",
      "Epoch 12/400\n",
      "511/521 [============================>.] - ETA: 0s - loss: 0.2421 - accuracy: 0.9166\n",
      "Epoch 00012: val_loss did not improve from 0.70534\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.2433 - accuracy: 0.9160 - val_loss: 0.7255 - val_accuracy: 0.7983\n",
      "Epoch 00012: early stopping\n",
      "443/443 [==============================] - 1s 2ms/step - loss: 0.7053 - accuracy: 0.7782\n",
      "Score for fold 2: loss of 0.7053379416465759; accuracy of 77.82169580459595%\n",
      "Target label num is 1667, and total label num is 16669\n",
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521/521 [==============================] - ETA: 0s - loss: 1.5830 - accuracy: 0.4303\n",
      "Epoch 00001: val_loss improved from inf to 1.65047, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 4s 7ms/step - loss: 1.5830 - accuracy: 0.4303 - val_loss: 1.6505 - val_accuracy: 0.4232\n",
      "Epoch 2/400\n",
      "511/521 [============================>.] - ETA: 0s - loss: 1.2166 - accuracy: 0.5637\n",
      "Epoch 00002: val_loss improved from 1.65047 to 1.25308, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 1.2154 - accuracy: 0.5645 - val_loss: 1.2531 - val_accuracy: 0.5643\n",
      "Epoch 3/400\n",
      "514/521 [============================>.] - ETA: 0s - loss: 1.0308 - accuracy: 0.6304\n",
      "Epoch 00003: val_loss improved from 1.25308 to 1.07344, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 1.0316 - accuracy: 0.6305 - val_loss: 1.0734 - val_accuracy: 0.6175\n",
      "Epoch 4/400\n",
      "517/521 [============================>.] - ETA: 0s - loss: 0.8931 - accuracy: 0.6858\n",
      "Epoch 00004: val_loss improved from 1.07344 to 1.00673, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.8929 - accuracy: 0.6860 - val_loss: 1.0067 - val_accuracy: 0.6564\n",
      "Epoch 5/400\n",
      "513/521 [============================>.] - ETA: 0s - loss: 0.7776 - accuracy: 0.7241\n",
      "Epoch 00005: val_loss improved from 1.00673 to 0.97573, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.7776 - accuracy: 0.7242 - val_loss: 0.9757 - val_accuracy: 0.6756\n",
      "Epoch 6/400\n",
      "512/521 [============================>.] - ETA: 0s - loss: 0.6673 - accuracy: 0.7679\n",
      "Epoch 00006: val_loss improved from 0.97573 to 0.84653, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.6658 - accuracy: 0.7687 - val_loss: 0.8465 - val_accuracy: 0.7238\n",
      "Epoch 7/400\n",
      "519/521 [============================>.] - ETA: 0s - loss: 0.5804 - accuracy: 0.7959\n",
      "Epoch 00007: val_loss improved from 0.84653 to 0.81799, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.5808 - accuracy: 0.7958 - val_loss: 0.8180 - val_accuracy: 0.7248\n",
      "Epoch 8/400\n",
      "515/521 [============================>.] - ETA: 0s - loss: 0.4809 - accuracy: 0.8315\n",
      "Epoch 00008: val_loss did not improve from 0.81799\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.4821 - accuracy: 0.8311 - val_loss: 1.0230 - val_accuracy: 0.6975\n",
      "Epoch 9/400\n",
      "516/521 [============================>.] - ETA: 0s - loss: 0.3985 - accuracy: 0.8606\n",
      "Epoch 00009: val_loss did not improve from 0.81799\n",
      "521/521 [==============================] - 3s 7ms/step - loss: 0.3996 - accuracy: 0.8602 - val_loss: 0.8426 - val_accuracy: 0.7377\n",
      "Epoch 10/400\n",
      "513/521 [============================>.] - ETA: 0s - loss: 0.3333 - accuracy: 0.8798\n",
      "Epoch 00010: val_loss improved from 0.81799 to 0.74451, saving model to ../models/CNN\\Model_015_3_Best.hdf5\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.3342 - accuracy: 0.8793 - val_loss: 0.7445 - val_accuracy: 0.7759\n",
      "Epoch 11/400\n",
      "510/521 [============================>.] - ETA: 0s - loss: 0.2667 - accuracy: 0.9057\n",
      "Epoch 00011: val_loss did not improve from 0.74451\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.2695 - accuracy: 0.9047 - val_loss: 1.0114 - val_accuracy: 0.7334\n",
      "Epoch 12/400\n",
      "511/521 [============================>.] - ETA: 0s - loss: 0.2236 - accuracy: 0.9212\n",
      "Epoch 00012: val_loss did not improve from 0.74451\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.2237 - accuracy: 0.9214 - val_loss: 0.8893 - val_accuracy: 0.7784\n",
      "Epoch 13/400\n",
      "519/521 [============================>.] - ETA: 0s - loss: 0.1991 - accuracy: 0.9306\n",
      "Epoch 00013: val_loss did not improve from 0.74451\n",
      "521/521 [==============================] - 3s 6ms/step - loss: 0.1995 - accuracy: 0.9306 - val_loss: 0.8393 - val_accuracy: 0.7934\n",
      "Epoch 00013: early stopping\n",
      "443/443 [==============================] - 1s 2ms/step - loss: 0.7445 - accuracy: 0.7759\n",
      "Score for fold 3: loss of 0.7445077300071716; accuracy of 77.59423851966858%\n",
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cifar10_labels = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck']\n",
    "\n",
    "# train\n",
    "saveDir = \"../models/CNN/\"\n",
    "histories = []\n",
    "nb_classes = 10\n",
    "predicts = np.zeros((10000, 10))\n",
    "cv_acc = 0\n",
    "cv_f1 = 0 \n",
    "\n",
    "# cross validation\n",
    "# Define the K-fold Cross Validator\n",
    "n_splits = 3\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "fold_no = 1\n",
    "for train_index, test_index in kfold.split(x_train_removed, y_train_removed):\n",
    "    # model instance\n",
    "    model01 = create_StackedAE01_CNN01_model(stack01.encoder)\n",
    "    adam = Adam() # defalut\n",
    "    model01.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics = [\"accuracy\"])\n",
    "    \n",
    "    # get target label num\n",
    "    y_train_ = y_train_removed[train_index]\n",
    "    df_train = pd.DataFrame(y_train_, columns=['label'])\n",
    "    target_index = df_train[df_train['label'] == cifar10_labels.index('bird')].index # bird label\n",
    "    limit_num = len(target_index)\n",
    "\n",
    "    # concat nate for under sampling\n",
    "    train_index_undersampling = np.concatenate([\n",
    "        # target label\n",
    "        df_train[df_train['label'] == cifar10_labels.index('bird')].index.values, # bird\n",
    "        df_train[df_train['label'] == cifar10_labels.index('deer')].index.values, # deer\n",
    "        df_train[df_train['label'] == cifar10_labels.index('truck')].index.values, # truck\n",
    "        # other label\n",
    "        df_train[df_train['label'] == cifar10_labels.index('airplane')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('automobile')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('cat')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('dog')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('frog')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('horse')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('ship')].sample(limit_num, random_state=42).index.values\n",
    "    ])\n",
    "\n",
    "    print(f'Target label num is {limit_num}, and total label num is {len(train_index_undersampling)}')\n",
    "    \n",
    "    x_train_ = x_train_removed[train_index_undersampling]\n",
    "    y_train_ = y_train_removed[train_index_undersampling]\n",
    "    \n",
    "    x_valid_ = x_train_removed[test_index]\n",
    "    y_valid_ = y_train_removed[test_index]\n",
    "\n",
    "    # one hot encoding\n",
    "    y_train_onehot = to_categorical(y_train_, nb_classes)\n",
    "    y_valid_onehot = to_categorical(y_valid_, nb_classes)\n",
    "    y_test_onehot = to_categorical(y_test, nb_classes)\n",
    "    \n",
    "    # callback\n",
    "    es_cb = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    chkpt = saveDir + 'Model_015_' +  str(fold_no) + '_Best.hdf5'\n",
    "    cp_cb = ModelCheckpoint(filepath = chkpt, \\\n",
    "       monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    model01_history = model01.fit(x_train_, y_train_onehot,\n",
    "                          batch_size=32,\n",
    "                          epochs=400,\n",
    "                          verbose=1,\n",
    "                          validation_data=(x_valid_, y_valid_onehot),\n",
    "                          callbacks=[es_cb, cp_cb],\n",
    "                          shuffle=True)\n",
    "    \n",
    "    # inference\n",
    "    model01.load_weights(chkpt)\n",
    "    scores = model01.evaluate(x_valid_, y_valid_onehot)\n",
    "    \n",
    "    # CV value\n",
    "    cv_acc += scores[1]*100\n",
    "    y_valid_pred =  model01.predict(x_valid_)\n",
    "    y_valid_pred = np.argmax(y_valid_pred, axis=1)\n",
    "    cv_f1 += f1_score(y_valid_, y_valid_pred, average='macro')*100\n",
    "    \n",
    "    print(f'Score for fold {fold_no}: {model01.metrics_names[0]} of {scores[0]}; {model01.metrics_names[1]} of {scores[1]*100}%')\n",
    "    \n",
    "    predict = model01.predict(x_test)\n",
    "    predicts += predict\n",
    "    \n",
    "    histories.append(model01_history.history)\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.86      0.78      1000\n",
      "           1       0.89      0.86      0.87      1000\n",
      "           2       0.73      0.59      0.65      1000\n",
      "           3       0.63      0.58      0.60      1000\n",
      "           4       0.70      0.72      0.71      1000\n",
      "           5       0.72      0.66      0.69      1000\n",
      "           6       0.70      0.85      0.77      1000\n",
      "           7       0.81      0.83      0.82      1000\n",
      "           8       0.90      0.84      0.87      1000\n",
      "           9       0.87      0.83      0.85      1000\n",
      "\n",
      "    accuracy                           0.76     10000\n",
      "   macro avg       0.76      0.76      0.76     10000\n",
      "weighted avg       0.76      0.76      0.76     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_histories = histories\n",
    "ensemble_predicts = predicts\n",
    "ensemble_predicts_ = ensemble_predicts / n_splits\n",
    "y_pred = np.argmax(ensemble_predicts_, axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ACC is 76.0,n_splits CV macro F1 is 75.0\n"
     ]
    }
   ],
   "source": [
    "print(f'CV ACC is {cv_acc//n_splits},n_splits CV macro F1 is {cv_f1//n_splits}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under sampling with data augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label num is 1666, and total label num is 16661\n",
      "WARNING:tensorflow:From <timed exec>:97: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "Epoch 1/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 1.6067 - accuracy: 0.4196\n",
      "Epoch 00001: val_loss improved from inf to 2.15556, saving model to ../models/CNN\\Model_016_1_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.6062 - accuracy: 0.4196 - val_loss: 2.1556 - val_accuracy: 0.3860\n",
      "Epoch 2/400\n",
      "515/520 [============================>.] - ETA: 0s - loss: 1.3045 - accuracy: 0.5338\n",
      "Epoch 00002: val_loss improved from 2.15556 to 1.72103, saving model to ../models/CNN\\Model_016_1_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.3038 - accuracy: 0.5343 - val_loss: 1.7210 - val_accuracy: 0.4796\n",
      "Epoch 3/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 1.1379 - accuracy: 0.5951\n",
      "Epoch 00003: val_loss improved from 1.72103 to 1.18704, saving model to ../models/CNN\\Model_016_1_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.1362 - accuracy: 0.5953 - val_loss: 1.1870 - val_accuracy: 0.5846\n",
      "Epoch 4/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 1.0003 - accuracy: 0.6446\n",
      "Epoch 00004: val_loss improved from 1.18704 to 1.10311, saving model to ../models/CNN\\Model_016_1_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.0004 - accuracy: 0.6447 - val_loss: 1.1031 - val_accuracy: 0.6343\n",
      "Epoch 5/400\n",
      "516/520 [============================>.] - ETA: 0s - loss: 0.9051 - accuracy: 0.6761\n",
      "Epoch 00005: val_loss improved from 1.10311 to 0.90418, saving model to ../models/CNN\\Model_016_1_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.9054 - accuracy: 0.6765 - val_loss: 0.9042 - val_accuracy: 0.6884\n",
      "Epoch 6/400\n",
      "515/520 [============================>.] - ETA: 0s - loss: 0.8320 - accuracy: 0.7072\n",
      "Epoch 00006: val_loss did not improve from 0.90418\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.8322 - accuracy: 0.7072 - val_loss: 1.0044 - val_accuracy: 0.6547\n",
      "Epoch 7/400\n",
      "516/520 [============================>.] - ETA: 0s - loss: 0.7608 - accuracy: 0.7297\n",
      "Epoch 00007: val_loss did not improve from 0.90418\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7623 - accuracy: 0.7292 - val_loss: 1.0127 - val_accuracy: 0.6621\n",
      "Epoch 8/400\n",
      "514/520 [============================>.] - ETA: 0s - loss: 0.7103 - accuracy: 0.7520\n",
      "Epoch 00008: val_loss did not improve from 0.90418\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7093 - accuracy: 0.7521 - val_loss: 0.9394 - val_accuracy: 0.6899\n",
      "Epoch 00008: early stopping\n",
      "443/443 [==============================] - 1s 3ms/step - loss: 0.9040 - accuracy: 0.6885\n",
      "Score for fold 1: loss of 0.9040382504463196; accuracy of 68.85014176368713%\n",
      "Target label num is 1667, and total label num is 16670\n",
      "Epoch 1/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 1.6072 - accuracy: 0.4194\n",
      "Epoch 00001: val_loss improved from inf to 1.44275, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.6075 - accuracy: 0.4190 - val_loss: 1.4428 - val_accuracy: 0.4747\n",
      "Epoch 2/400\n",
      "520/520 [==============================] - ETA: 0s - loss: 1.2862 - accuracy: 0.5368\n",
      "Epoch 00002: val_loss improved from 1.44275 to 1.37312, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.2862 - accuracy: 0.5368 - val_loss: 1.3731 - val_accuracy: 0.5125\n",
      "Epoch 3/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 1.1197 - accuracy: 0.6060\n",
      "Epoch 00003: val_loss improved from 1.37312 to 1.08702, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.1201 - accuracy: 0.6059 - val_loss: 1.0870 - val_accuracy: 0.6205\n",
      "Epoch 4/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.9989 - accuracy: 0.6477\n",
      "Epoch 00004: val_loss did not improve from 1.08702\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.9993 - accuracy: 0.6475 - val_loss: 1.0942 - val_accuracy: 0.6274\n",
      "Epoch 5/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.8974 - accuracy: 0.6767\n",
      "Epoch 00005: val_loss improved from 1.08702 to 1.04722, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.8972 - accuracy: 0.6768 - val_loss: 1.0472 - val_accuracy: 0.6485\n",
      "Epoch 6/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.8365 - accuracy: 0.7022\n",
      "Epoch 00006: val_loss improved from 1.04722 to 0.93397, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.8367 - accuracy: 0.7019 - val_loss: 0.9340 - val_accuracy: 0.6788\n",
      "Epoch 7/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 0.7563 - accuracy: 0.7301\n",
      "Epoch 00007: val_loss improved from 0.93397 to 0.88783, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7565 - accuracy: 0.7298 - val_loss: 0.8878 - val_accuracy: 0.6969\n",
      "Epoch 8/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.7079 - accuracy: 0.7512\n",
      "Epoch 00008: val_loss improved from 0.88783 to 0.74672, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7077 - accuracy: 0.7512 - val_loss: 0.7467 - val_accuracy: 0.7421\n",
      "Epoch 9/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 0.6675 - accuracy: 0.7657\n",
      "Epoch 00009: val_loss did not improve from 0.74672\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.6679 - accuracy: 0.7655 - val_loss: 1.2808 - val_accuracy: 0.6273\n",
      "Epoch 10/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.6212 - accuracy: 0.7804\n",
      "Epoch 00010: val_loss improved from 0.74672 to 0.74264, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.6218 - accuracy: 0.7804 - val_loss: 0.7426 - val_accuracy: 0.7454\n",
      "Epoch 11/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.5776 - accuracy: 0.7970\n",
      "Epoch 00011: val_loss did not improve from 0.74264\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5778 - accuracy: 0.7971 - val_loss: 0.8437 - val_accuracy: 0.7207\n",
      "Epoch 12/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.5462 - accuracy: 0.8044\n",
      "Epoch 00012: val_loss did not improve from 0.74264\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5458 - accuracy: 0.8046 - val_loss: 0.8801 - val_accuracy: 0.7338\n",
      "Epoch 13/400\n",
      "516/520 [============================>.] - ETA: 0s - loss: 0.5182 - accuracy: 0.8180\n",
      "Epoch 00013: val_loss improved from 0.74264 to 0.70157, saving model to ../models/CNN\\Model_016_2_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5186 - accuracy: 0.8180 - val_loss: 0.7016 - val_accuracy: 0.7751\n",
      "Epoch 14/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 0.4833 - accuracy: 0.8309\n",
      "Epoch 00014: val_loss did not improve from 0.70157\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.4828 - accuracy: 0.8309 - val_loss: 0.7227 - val_accuracy: 0.7754\n",
      "Epoch 15/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.4580 - accuracy: 0.8363\n",
      "Epoch 00015: val_loss did not improve from 0.70157\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.4583 - accuracy: 0.8365 - val_loss: 0.7746 - val_accuracy: 0.7519\n",
      "Epoch 16/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.4276 - accuracy: 0.8466\n",
      "Epoch 00016: val_loss did not improve from 0.70157\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.4278 - accuracy: 0.8463 - val_loss: 0.7785 - val_accuracy: 0.7623\n",
      "Epoch 00016: early stopping\n",
      "443/443 [==============================] - 1s 3ms/step - loss: 0.7010 - accuracy: 0.7753\n",
      "Score for fold 2: loss of 0.7010352611541748; accuracy of 77.52523422241211%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target label num is 1667, and total label num is 16669\n",
      "Epoch 1/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 1.6176 - accuracy: 0.4200\n",
      "Epoch 00001: val_loss improved from inf to 1.67252, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.6171 - accuracy: 0.4198 - val_loss: 1.6725 - val_accuracy: 0.4256\n",
      "Epoch 2/400\n",
      "514/520 [============================>.] - ETA: 0s - loss: 1.2783 - accuracy: 0.5393\n",
      "Epoch 00002: val_loss improved from 1.67252 to 1.52874, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.2781 - accuracy: 0.5393 - val_loss: 1.5287 - val_accuracy: 0.5093\n",
      "Epoch 3/400\n",
      "515/520 [============================>.] - ETA: 0s - loss: 1.1205 - accuracy: 0.6006\n",
      "Epoch 00003: val_loss improved from 1.52874 to 1.43196, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 1.1192 - accuracy: 0.6009 - val_loss: 1.4320 - val_accuracy: 0.4914\n",
      "Epoch 4/400\n",
      "516/520 [============================>.] - ETA: 0s - loss: 0.9995 - accuracy: 0.6471\n",
      "Epoch 00004: val_loss did not improve from 1.43196\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.9999 - accuracy: 0.6472 - val_loss: 1.7024 - val_accuracy: 0.5033\n",
      "Epoch 5/400\n",
      "514/520 [============================>.] - ETA: 0s - loss: 0.9144 - accuracy: 0.6750\n",
      "Epoch 00005: val_loss improved from 1.43196 to 0.99867, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.9122 - accuracy: 0.6759 - val_loss: 0.9987 - val_accuracy: 0.6656\n",
      "Epoch 6/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.8297 - accuracy: 0.7067\n",
      "Epoch 00006: val_loss did not improve from 0.99867\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.8297 - accuracy: 0.7066 - val_loss: 1.1431 - val_accuracy: 0.6392\n",
      "Epoch 7/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.7624 - accuracy: 0.7305\n",
      "Epoch 00007: val_loss improved from 0.99867 to 0.92241, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7619 - accuracy: 0.7307 - val_loss: 0.9224 - val_accuracy: 0.6816\n",
      "Epoch 8/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.7089 - accuracy: 0.7483\n",
      "Epoch 00008: val_loss did not improve from 0.92241\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.7087 - accuracy: 0.7482 - val_loss: 0.9944 - val_accuracy: 0.6893\n",
      "Epoch 9/400\n",
      "517/520 [============================>.] - ETA: 0s - loss: 0.6713 - accuracy: 0.7653\n",
      "Epoch 00009: val_loss improved from 0.92241 to 0.78508, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.6712 - accuracy: 0.7652 - val_loss: 0.7851 - val_accuracy: 0.7392\n",
      "Epoch 10/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.6175 - accuracy: 0.7798\n",
      "Epoch 00010: val_loss improved from 0.78508 to 0.68624, saving model to ../models/CNN\\Model_016_3_Best.hdf5\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.6177 - accuracy: 0.7798 - val_loss: 0.6862 - val_accuracy: 0.7647\n",
      "Epoch 11/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.5816 - accuracy: 0.7936\n",
      "Epoch 00011: val_loss did not improve from 0.68624\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5815 - accuracy: 0.7937 - val_loss: 0.8425 - val_accuracy: 0.7228\n",
      "Epoch 12/400\n",
      "518/520 [============================>.] - ETA: 0s - loss: 0.5387 - accuracy: 0.8081\n",
      "Epoch 00012: val_loss did not improve from 0.68624\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5383 - accuracy: 0.8083 - val_loss: 0.7902 - val_accuracy: 0.7460\n",
      "Epoch 13/400\n",
      "519/520 [============================>.] - ETA: 0s - loss: 0.5132 - accuracy: 0.8178\n",
      "Epoch 00013: val_loss did not improve from 0.68624\n",
      "520/520 [==============================] - 5s 10ms/step - loss: 0.5132 - accuracy: 0.8178 - val_loss: 0.7006 - val_accuracy: 0.7690\n",
      "Epoch 00013: early stopping\n",
      "443/443 [==============================] - 1s 3ms/step - loss: 0.6860 - accuracy: 0.7649\n",
      "Score for fold 3: loss of 0.6860131621360779; accuracy of 76.48594975471497%\n",
      "Wall time: 3min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cifar10_labels = [\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck']\n",
    "\n",
    "# train\n",
    "saveDir = \"../models/CNN/\"\n",
    "histories = []\n",
    "nb_classes = 10\n",
    "predicts = np.zeros((10000, 10))\n",
    "cv_acc = 0\n",
    "cv_f1 = 0 \n",
    "\n",
    "# cross validation\n",
    "# Define the K-fold Cross Validator\n",
    "n_splits = 3\n",
    "kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "fold_no = 1\n",
    "for train_index, test_index in kfold.split(x_train_removed, y_train_removed):\n",
    "    # model instance\n",
    "    model02 = create_StackedAE01_CNN01_model(stack01.encoder)\n",
    "    adam = Adam() # defalut\n",
    "    model02.compile(loss = \"categorical_crossentropy\", optimizer = adam, metrics = [\"accuracy\"])\n",
    "\n",
    "    # get target label num\n",
    "    y_train_ = y_train_removed[train_index]\n",
    "    df_train = pd.DataFrame(y_train_, columns=['label'])\n",
    "    target_index = df_train[df_train['label'] == cifar10_labels.index('bird')].index # bird label\n",
    "    limit_num = len(target_index)\n",
    "\n",
    "    # concat nate for under sampling\n",
    "    train_index_undersampling = np.concatenate([\n",
    "        # target label\n",
    "        df_train[df_train['label'] == cifar10_labels.index('bird')].index.values, # bird\n",
    "        df_train[df_train['label'] == cifar10_labels.index('deer')].index.values, # deer\n",
    "        df_train[df_train['label'] == cifar10_labels.index('truck')].index.values, # truck\n",
    "        # other label\n",
    "        df_train[df_train['label'] == cifar10_labels.index('airplane')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('automobile')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('cat')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('dog')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('frog')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('horse')].sample(limit_num, random_state=42).index.values,\n",
    "        df_train[df_train['label'] == cifar10_labels.index('ship')].sample(limit_num, random_state=42).index.values\n",
    "    ])\n",
    "\n",
    "    print(f'Target label num is {limit_num}, and total label num is {len(train_index_undersampling)}')\n",
    "    \n",
    "    x_train_ = x_train_removed[train_index_undersampling]\n",
    "    y_train_ = y_train_removed[train_index_undersampling]\n",
    "    \n",
    "    x_valid_ = x_train_removed[test_index]\n",
    "    y_valid_ = y_train_removed[test_index]\n",
    "\n",
    "    # one hot encoding\n",
    "    y_train_onehot = to_categorical(y_train_, nb_classes)\n",
    "    y_valid_onehot = to_categorical(y_valid_, nb_classes)\n",
    "    y_test_onehot = to_categorical(y_test, nb_classes)\n",
    "    \n",
    "    # callback\n",
    "    es_cb = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')\n",
    "    chkpt = saveDir + 'Model_016_' +  str(fold_no) + '_Best.hdf5'\n",
    "    cp_cb = ModelCheckpoint(filepath = chkpt, \\\n",
    "       monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
    "    \n",
    "    # create generator\n",
    "    train_datagen = ImageDataGenerator(\n",
    "#         rescale=1./255,\n",
    "#         rotation_range=10,\n",
    "#         shear_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "#         vertical_flip=True,\n",
    "#         width_shift_range=0.1,\n",
    "#         height_shift_range=0.1,\n",
    "        zoom_range=0.1\n",
    "#         channel_shift_range=0.2\n",
    "        )\n",
    "\n",
    "    batch_size = 32\n",
    "    train_datagenerator = train_datagen.flow(x_train_, y_train_onehot, batch_size)\n",
    "    valid_datagenerator = ImageDataGenerator().flow(x_valid_, y_valid_onehot, batch_size)\n",
    "\n",
    "    model02_history = model02.fit_generator(train_datagenerator,\n",
    "                                  steps_per_epoch=int(len(x_train_)//batch_size),\n",
    "                                  epochs=400,\n",
    "                                  validation_data=valid_datagenerator,\n",
    "                                  validation_steps=int(len(x_valid_)//batch_size),\n",
    "                                  verbose=1,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=[es_cb, cp_cb])\n",
    "\n",
    "    # inference\n",
    "    model02.load_weights(chkpt)\n",
    "    scores = model02.evaluate(x_valid_, y_valid_onehot)\n",
    "    \n",
    "    # CV value\n",
    "    cv_acc += scores[1]*100\n",
    "    y_valid_pred =  model02.predict(x_valid_)\n",
    "    y_valid_pred = np.argmax(y_valid_pred, axis=1)\n",
    "    cv_f1 += f1_score(y_valid_, y_valid_pred, average='macro')*100\n",
    "    \n",
    "    print(f'Score for fold {fold_no}: {model02.metrics_names[0]} of {scores[0]}; {model02.metrics_names[1]} of {scores[1]*100}%')\n",
    "    \n",
    "    predict = model02.predict(x_test)\n",
    "    predicts += predict\n",
    "    \n",
    "    histories.append(model02_history.history)\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.82      0.78      1000\n",
      "           1       0.82      0.93      0.87      1000\n",
      "           2       0.75      0.64      0.69      1000\n",
      "           3       0.75      0.48      0.59      1000\n",
      "           4       0.82      0.69      0.75      1000\n",
      "           5       0.68      0.79      0.73      1000\n",
      "           6       0.71      0.89      0.79      1000\n",
      "           7       0.80      0.85      0.83      1000\n",
      "           8       0.91      0.82      0.86      1000\n",
      "           9       0.83      0.87      0.85      1000\n",
      "\n",
      "    accuracy                           0.78     10000\n",
      "   macro avg       0.78      0.78      0.77     10000\n",
      "weighted avg       0.78      0.78      0.77     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ensemble_dataaug_histories = histories\n",
    "ensemble_dataaug_predicts = predicts\n",
    "ensemble_dataaug_predicts_ = ensemble_dataaug_predicts / n_splits\n",
    "y_pred = np.argmax(ensemble_dataaug_predicts_, axis=1)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ACC is 74.0,n_splits CV macro F1 is 72.0\n"
     ]
    }
   ],
   "source": [
    "print(f'CV ACC is {cv_acc//n_splits},n_splits CV macro F1 is {cv_f1//n_splits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
